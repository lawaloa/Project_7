{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3f0e974-faf8-458f-bf2a-06a469d0ea5e",
   "metadata": {},
   "source": [
    "![car](car.jpg)\n",
    "\n",
    "Insurance companies invest a lot of [time and money](https://www.accenture.com/_acnmedia/pdf-84/accenture-machine-leaning-insurance.pdf) into optimizing their pricing and accurately estimating the likelihood that customers will make a claim. In many countries insurance it is a legal requirement to have car insurance in order to drive a vehicle on public roads, so the market is very large!\n",
    "\n",
    "Knowing all of this, On the Road car insurance have requested your services in building a model to predict whether a customer will make a claim on their insurance during the policy period. As they have very little expertise and infrastructure for deploying and monitoring machine learning models, they've asked you to identify the single feature that results in the best performing model, as measured by accuracy, so they can start with a simple model in production.\n",
    "\n",
    "They have supplied you with their customer data as a csv file called `car_insurance.csv`, along with a table detailing the column names and descriptions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928ffdf-25d6-4ad9-909f-0dd8d10b9a42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The dataset\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `id` | Unique client identifier |\n",
    "| `age` | Client's age: <br> <ul><li>`0`: 16-25</li><li>`1`: 26-39</li><li>`2`: 40-64</li><li>`3`: 65+</li></ul> |\n",
    "| `gender` | Client's gender: <br> <ul><li>`0`: Female</li><li>`1`: Male</li></ul> |\n",
    "| `driving_experience` | Years the client has been driving: <br> <ul><li>`0`: 0-9</li><li>`1`: 10-19</li><li>`2`: 20-29</li><li>`3`: 30+</li></ul> |\n",
    "| `education` | Client's level of education: <br> <ul><li>`0`: No education</li><li>`1`: High school</li><li>`2`: University</li></ul> |\n",
    "| `income` | Client's income level: <br> <ul><li>`0`: Poverty</li><li>`1`: Working class</li><li>`2`: Middle class</li><li>`3`: Upper class</li></ul> |\n",
    "| `credit_score` | Client's credit score (between zero and one) |\n",
    "| `vehicle_ownership` | Client's vehicle ownership status: <br><ul><li>`0`: Does not own their vehilce (paying off finance)</li><li>`1`: Owns their vehicle</li></ul> |\n",
    "| `vehcile_year` | Year of vehicle registration: <br><ul><li>`0`: Before 2015</li><li>`1`: 2015 or later</li></ul> |\n",
    "| `married` | Client's marital status: <br><ul><li>`0`: Not married</li><li>`1`: Married</li></ul> |\n",
    "| `children` | Client's number of children |\n",
    "| `postal_code` | Client's postal code | \n",
    "| `annual_mileage` | Number of miles driven by the client each year |\n",
    "| `vehicle_type` | Type of car: <br> <ul><li>`0`: Sedan</li><li>`1`: Sports car</li></ul> |\n",
    "| `speeding_violations` | Total number of speeding violations received by the client | \n",
    "| `duis` | Number of times the client has been caught driving under the influence of alcohol |\n",
    "| `past_accidents` | Total number of previous accidents the client has been involved in |\n",
    "| `outcome` | Whether the client made a claim on their car insurance (response variable): <br><ul><li>`0`: No claim</li><li>`1`: Made a claim</li></ul> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2441,
    "id": "bA5ajAmk7XH6",
    "lastExecutedAt": 1732296634295,
    "lastExecutedByKernel": "228849a6-3b2c-438c-aba9-acebd191be1b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\n\n# Start coding!\ncar = pd.read_csv(\"car_insurance.csv\")\n\n# Exploring the data\n\ndef explore(data, head_rows=5, group_by_col=None, cols_to_impute=None):\n    \"\"\"\n    Explores the given DataFrame by displaying basic information, summary statistics, \n    the first few rows, unique values, and imputes missing values with group means if specified.\n\n    Parameters:\n        data (pd.DataFrame): The DataFrame to explore.\n        head_rows (int): Number of rows to display for the head of the DataFrame. Default is 5.\n        group_by_col (str): Column name to group by for imputing missing values. Default is None.\n        cols_to_impute (list): List of column names to impute missing values. Default is None.\n    \"\"\"\n    print(\"\\n--- DataFrame Info ---\\n\")\n    data.info()\n    \n    print(\"\\n--- Summary Statistics ---\\n\")\n    print(data.describe(include='all'))  # Include all data types in describe()\n    \n    print(f\"\\n--- First {head_rows} Rows ---\\n\")\n    print(data.head(head_rows))\n    \n    print(\"\\n--- Unique Values ---\\n\")\n    for col in data.columns:\n        print(f\"{col}: {data[col].unique()}\")\n    \n    # Impute missing values if group_by_col and cols_to_impute are specified\n    if group_by_col and cols_to_impute:\n        print(\"\\n--- Imputing Missing Values ---\\n\")\n        try:\n            group_means = data.groupby(group_by_col)[cols_to_impute].mean().to_dict()  # Group means as a dictionary\n            for col in cols_to_impute:\n                if col in data.columns:\n                    print(f\"Imputing missing values in '{col}' based on group means of '{group_by_col}'\")\n                    data[col] = data[col].fillna(data[group_by_col].map(group_means[col]))\n                else:\n                    print(f\"Column '{col}' not found in the dataset.\")\n        except Exception as e:\n            print(f\"Error while imputing missing values: {e}\")\n    \n    print(\"\\n--- Any missing values again ? ---\\n\")\n    print(data.isna().sum())\n# Example usage\n# Replace `your_data` with your actual DataFrame\n# explore(your_data, group_by_col=\"outcome\", cols_to_impute=[\"credit_score\", \"annual_mileage\"])\n\nexplore(car, group_by_col=\"outcome\", cols_to_impute=[\"credit_score\", \"annual_mileage\"])\n\ndef find_best_feature_statsmodels(data, outcome_column='outcome', id_column='id'):\n    \"\"\"\n    Identifies the single best feature for predicting the outcome column using logistic regression \n    with statsmodels. Calculates accuracy directly from the confusion matrix.\n\n    Parameters:\n        data (pd.DataFrame): The dataset containing features and the outcome column.\n        outcome_column (str): The name of the target column.\n        id_column (str): The name of the column to exclude from analysis.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the best feature and its accuracy score.\n    \"\"\"\n    # Exclude ID and outcome columns from feature set\n    data1 = data.drop(columns=[id_column, outcome_column])\n    \n    best_feature = None\n    best_accuracy = 0\n\n    # Iterate through each feature\n    for col in data1.columns:\n        # Create formula for logistic regression\n        formula = f\"{outcome_column} ~ {col}\"\n        \n        # Fit logistic regression model on the entire dataset\n        model = logit(formula=formula, data=data).fit(disp=False)\n        \n        # Generate confusion matrix using pred_table()\n        confusion_matrix = model.pred_table()\n        \n        # Calculate accuracy from confusion matrix\n        TP = confusion_matrix[1, 1]\n        TN = confusion_matrix[0, 0]\n        T = confusion_matrix.sum()\n        accuracy = (TP + TN) / T\n\n        # Update the best feature if this one is better\n        if accuracy > best_accuracy:\n            best_feature = col\n            best_accuracy = accuracy\n\n    # Store results in a DataFrame\n    best_feature_df = pd.DataFrame({\n        \"best_feature\": [best_feature],\n        \"best_accuracy\": [best_accuracy]\n    })\n\n    return best_feature_df\n\n# Example usage\n# Replace `your_data` with your actual DataFrame\n# best_feature_df = find_best_feature_statsmodels(your_data)\n# print(best_feature_df)\n\nbest_feature_df = find_best_feature_statsmodels(car)\n\nprint(best_feature_df)\n",
    "outputsMetadata": {
     "0": {
      "height": 482,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame Info ---\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   10000 non-null  int64  \n",
      " 1   age                  10000 non-null  int64  \n",
      " 2   gender               10000 non-null  int64  \n",
      " 3   driving_experience   10000 non-null  object \n",
      " 4   education            10000 non-null  object \n",
      " 5   income               10000 non-null  object \n",
      " 6   credit_score         9018 non-null   float64\n",
      " 7   vehicle_ownership    10000 non-null  float64\n",
      " 8   vehicle_year         10000 non-null  object \n",
      " 9   married              10000 non-null  float64\n",
      " 10  children             10000 non-null  float64\n",
      " 11  postal_code          10000 non-null  int64  \n",
      " 12  annual_mileage       9043 non-null   float64\n",
      " 13  vehicle_type         10000 non-null  object \n",
      " 14  speeding_violations  10000 non-null  int64  \n",
      " 15  duis                 10000 non-null  int64  \n",
      " 16  past_accidents       10000 non-null  int64  \n",
      " 17  outcome              10000 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "\n",
      "--- Summary Statistics ---\n",
      "\n",
      "                   id           age  ...  past_accidents       outcome\n",
      "count    10000.000000  10000.000000  ...    10000.000000  10000.000000\n",
      "unique            NaN           NaN  ...             NaN           NaN\n",
      "top               NaN           NaN  ...             NaN           NaN\n",
      "freq              NaN           NaN  ...             NaN           NaN\n",
      "mean    500521.906800      1.489500  ...        1.056300      0.313300\n",
      "std     290030.768758      1.025278  ...        1.652454      0.463858\n",
      "min        101.000000      0.000000  ...        0.000000      0.000000\n",
      "25%     249638.500000      1.000000  ...        0.000000      0.000000\n",
      "50%     501777.000000      1.000000  ...        0.000000      0.000000\n",
      "75%     753974.500000      2.000000  ...        2.000000      1.000000\n",
      "max     999976.000000      3.000000  ...       15.000000      1.000000\n",
      "\n",
      "[11 rows x 18 columns]\n",
      "\n",
      "--- First 5 Rows ---\n",
      "\n",
      "       id  age  gender  ... duis past_accidents outcome\n",
      "0  569520    3       0  ...    0              0     0.0\n",
      "1  750365    0       1  ...    0              0     1.0\n",
      "2  199901    0       0  ...    0              0     0.0\n",
      "3  478866    0       1  ...    0              0     0.0\n",
      "4  731664    1       1  ...    0              1     1.0\n",
      "\n",
      "[5 rows x 18 columns]\n",
      "\n",
      "--- Unique Values ---\n",
      "\n",
      "id: [569520 750365 199901 ... 468409 903459 442696]\n",
      "age: [3 0 1 2]\n",
      "gender: [0 1]\n",
      "driving_experience: ['0-9y' '10-19y' '20-29y' '30y+']\n",
      "education: ['high school' 'none' 'university']\n",
      "income: ['upper class' 'poverty' 'working class' 'middle class']\n",
      "credit_score: [0.62902731 0.35775712 0.49314579 ... 0.47094023 0.36418478 0.43522478]\n",
      "vehicle_ownership: [1. 0.]\n",
      "vehicle_year: ['after 2015' 'before 2015']\n",
      "married: [0. 1.]\n",
      "children: [1. 0.]\n",
      "postal_code: [10238 32765 92101 21217]\n",
      "annual_mileage: [12000. 16000. 11000. 13000. 14000. 10000.  8000.    nan 18000. 17000.\n",
      "  7000. 15000.  9000.  5000.  6000. 19000.  4000.  3000.  2000. 20000.\n",
      " 21000. 22000.]\n",
      "vehicle_type: ['sedan' 'sports car']\n",
      "speeding_violations: [ 0  2  3  7  6  4 10 13  1  5  9  8 12 11 15 17 19 18 16 14 22]\n",
      "duis: [0 2 1 3 4 5 6]\n",
      "past_accidents: [ 0  1  3  7  2  5  4  6  8 10 11  9 12 14 15]\n",
      "outcome: [0. 1.]\n",
      "\n",
      "--- Imputing Missing Values ---\n",
      "\n",
      "Imputing missing values in 'credit_score' based on group means of 'outcome'\n",
      "Imputing missing values in 'annual_mileage' based on group means of 'outcome'\n",
      "\n",
      "--- Any missing values again ? ---\n",
      "\n",
      "id                     0\n",
      "age                    0\n",
      "gender                 0\n",
      "driving_experience     0\n",
      "education              0\n",
      "income                 0\n",
      "credit_score           0\n",
      "vehicle_ownership      0\n",
      "vehicle_year           0\n",
      "married                0\n",
      "children               0\n",
      "postal_code            0\n",
      "annual_mileage         0\n",
      "vehicle_type           0\n",
      "speeding_violations    0\n",
      "duis                   0\n",
      "past_accidents         0\n",
      "outcome                0\n",
      "dtype: int64\n",
      "         best_feature  best_accuracy\n",
      "0  driving_experience         0.7771\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "# Start coding!\n",
    "car = pd.read_csv(\"car_insurance.csv\")\n",
    "\n",
    "# Exploring the data\n",
    "\n",
    "def explore(data, head_rows=5, group_by_col=None, cols_to_impute=None):\n",
    "    \"\"\"\n",
    "    Explores the given DataFrame by displaying basic information, summary statistics, \n",
    "    the first few rows, unique values, and imputes missing values with group means if specified.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame to explore.\n",
    "        head_rows (int): Number of rows to display for the head of the DataFrame. Default is 5.\n",
    "        group_by_col (str): Column name to group by for imputing missing values. Default is None.\n",
    "        cols_to_impute (list): List of column names to impute missing values. Default is None.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- DataFrame Info ---\\n\")\n",
    "    data.info()\n",
    "    \n",
    "    print(\"\\n--- Summary Statistics ---\\n\")\n",
    "    print(data.describe(include='all'))  # Include all data types in describe()\n",
    "    \n",
    "    print(f\"\\n--- First {head_rows} Rows ---\\n\")\n",
    "    print(data.head(head_rows))\n",
    "    \n",
    "    print(\"\\n--- Unique Values ---\\n\")\n",
    "    for col in data.columns:\n",
    "        print(f\"{col}: {data[col].unique()}\")\n",
    "    \n",
    "    # Impute missing values if group_by_col and cols_to_impute are specified\n",
    "    if group_by_col and cols_to_impute:\n",
    "        print(\"\\n--- Imputing Missing Values ---\\n\")\n",
    "        try:\n",
    "            group_means = data.groupby(group_by_col)[cols_to_impute].mean().to_dict()  # Group means as a dictionary\n",
    "            for col in cols_to_impute:\n",
    "                if col in data.columns:\n",
    "                    print(f\"Imputing missing values in '{col}' based on group means of '{group_by_col}'\")\n",
    "                    data[col] = data[col].fillna(data[group_by_col].map(group_means[col]))\n",
    "                else:\n",
    "                    print(f\"Column '{col}' not found in the dataset.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while imputing missing values: {e}\")\n",
    "    \n",
    "    print(\"\\n--- Any missing values again ? ---\\n\")\n",
    "    print(data.isna().sum())\n",
    "# Example usage\n",
    "# Replace `your_data` with your actual DataFrame\n",
    "# explore(your_data, group_by_col=\"outcome\", cols_to_impute=[\"credit_score\", \"annual_mileage\"])\n",
    "\n",
    "explore(car, group_by_col=\"outcome\", cols_to_impute=[\"credit_score\", \"annual_mileage\"])\n",
    "\n",
    "def find_best_feature_statsmodels(data, outcome_column='outcome', id_column='id'):\n",
    "    \"\"\"\n",
    "    Identifies the single best feature for predicting the outcome column using logistic regression \n",
    "    with statsmodels. Calculates accuracy directly from the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing features and the outcome column.\n",
    "        outcome_column (str): The name of the target column.\n",
    "        id_column (str): The name of the column to exclude from analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the best feature and its accuracy score.\n",
    "    \"\"\"\n",
    "    # Exclude ID and outcome columns from feature set\n",
    "    data1 = data.drop(columns=[id_column, outcome_column])\n",
    "    \n",
    "    best_feature = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for col in data1.columns:\n",
    "        # Create formula for logistic regression\n",
    "        formula = f\"{outcome_column} ~ {col}\"\n",
    "        \n",
    "        # Fit logistic regression model on the entire dataset\n",
    "        model = logit(formula=formula, data=data).fit(disp=False)\n",
    "        \n",
    "        # Generate confusion matrix using pred_table()\n",
    "        confusion_matrix = model.pred_table()\n",
    "        \n",
    "        # Calculate accuracy from confusion matrix\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        TN = confusion_matrix[0, 0]\n",
    "        T = confusion_matrix.sum()\n",
    "        accuracy = (TP + TN) / T\n",
    "\n",
    "        # Update the best feature if this one is better\n",
    "        if accuracy > best_accuracy:\n",
    "            best_feature = col\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    # Store results in a DataFrame\n",
    "    best_feature_df = pd.DataFrame({\n",
    "        \"best_feature\": [best_feature],\n",
    "        \"best_accuracy\": [best_accuracy]\n",
    "    })\n",
    "\n",
    "    return best_feature_df\n",
    "\n",
    "# Example usage\n",
    "# Replace `your_data` with your actual DataFrame\n",
    "# best_feature_df = find_best_feature_statsmodels(your_data)\n",
    "# print(best_feature_df)\n",
    "\n",
    "best_feature_df = find_best_feature_statsmodels(car)\n",
    "\n",
    "print(best_feature_df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
